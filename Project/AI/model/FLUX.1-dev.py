import sys
import json
import re
import torch
from diffusers import FluxPipeline

# (ì„ íƒ ì‚¬í•­) ollama ì¢…ë£Œ
import subprocess
subprocess.run('sudo pkill -f ollama', shell=True, check=False)

# ğŸ”¹ ë°›ì€ í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸ íŒŒì‹±
prompts = json.loads(sys.argv[1])  # ë¬¸ìì—´ë¡œ ëœ JSON ë¦¬ìŠ¤íŠ¸ â†’ Python ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

# ğŸ”¹ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model_id = "black-forest-labs/FLUX.1-dev"
pipe = FluxPipeline.from_pretrained(model_id, torch_dtype=torch.bfloat16)
pipe.enable_model_cpu_offload()
pipe = pipe.to("cuda")

# ğŸ”¹ í”„ë¡¬í”„íŠ¸ ë³¸ë¬¸ ì¶”ì¶œ í•¨ìˆ˜
def extract_prompt_text(prompt_block: str) -> str:
    match = re.search(r'\*\*í”„ë¡¬í”„íŠ¸:\*\*\n\n"(.*?)"', prompt_block, re.DOTALL)
    return match.group(1).strip() if match else prompt_block.strip()

# ğŸ”¹ í”„ë¡¬í”„íŠ¸ë³„ ì´ë¯¸ì§€ ìƒì„±
for idx, block in enumerate(prompts):
    prompt_text = extract_prompt_text(block)

    image = pipe(
        prompt=prompt_text,
        guidance_scale=7.5,
        num_inference_steps=50,
        max_sequence_length=512,
    generator=torch.Generator("cpu").manual_seed(0)
    ).images[0]

    image.save(f"./img_dir/stable-diffusion-xl-base-1.0/sd-image-step{idx+1}.png")
    print(f"âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: sd-image-step{idx+1}.png")
